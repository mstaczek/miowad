{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody Inteligencji Obliczeniowej w Analizie Danych\n",
    "\n",
    "[Tasks](https://pages.mini.pw.edu.pl/~karwowskij/mioad/lab-sieci.html)\n",
    "\n",
    "### Lab 6: NN5: Testowanie różnych funkcji aktywacji (1 tydzień, 2 punkty)\n",
    "\n",
    "Należy rozszerzyć istniejącą implementację sieci i metody uczącej o możliwość wyboru funkcji aktywacji:\n",
    "- sigmoid,\n",
    "- liniowa,\n",
    "- tanh,\n",
    "- ReLU.\n",
    "\n",
    "Pytanie dodatkowe – czy wszyscy implementują dokładnie gradient dla funkcji ReLU?\n",
    "\n",
    "\n",
    "\n",
    "Porównać szybkość uczenia i skuteczność sieci w zależności od liczby neuronów w poszczególnych warstwach i rodzaju funkcji aktywacji.\n",
    "\n",
    "Należy wziąć pod uwagę fakt, że różne funkcje aktywacji mogą dawać różną skuteczność w zależności od liczby neuronów i liczby warstw. \n",
    "\n",
    "**Sprawdzić sieci z jedną, dwiema i trzema warstwami ukrytymi.** Podobnie jak w poprzednim tygodniu, trzeba dostosować proces uczenia\n",
    "do pochodnych nowych funkcji aktywacji.\n",
    "\n",
    "**Przeprowadzić testy wstępne dla zbioru multimodal-large (regresja), dla wszystkich trzech architektur i wszystkich czterech funkcji aktywacji.**\n",
    "\n",
    "Dla pozostałych zbiorów wybrać dwa najlepsze zestawy i zbadać ich skuteczność:\n",
    "- regresja\n",
    "- - steps-large,\n",
    "- klasyfikacja\n",
    "- - rings5-regular\n",
    "- - rings3-regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "- ~~implement sigmoid, linear, tanh, ReLU functions~~\n",
    "- ~~answer additional question regarding ReLU gradient implementation~~\n",
    "- ~~compare speed of learning and accuracy of networks with different number of hidden layers and activation functions on multimodal-large dataset~~\n",
    "- ~~for each other dataset choose two best sets and test their accuracy~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MultiLayerPerceptron as mlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: multimodal-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x           y\n",
      "0  0.493292  -98.208166\n",
      "1 -0.470203  -55.283891\n",
      "2  1.869983  100.299997\n",
      "3 -1.040446    2.720629\n",
      "4 -0.616507  -75.991636\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/regression/multimodal-large-test.csv')\n",
    "df_test = pd.read_csv('data/regression/multimodal-large-training.csv')\n",
    "print(df_train.head())\n",
    "\n",
    "x_train = [[x] for x in df_train.loc[:,\"x\"]]\n",
    "y_train = [[y] for y in df_train.loc[:,\"y\"]]\n",
    "x_test = [[x] for x in df_test.loc[:,\"x\"]]\n",
    "y_test = [[y] for y in df_test.loc[:,\"y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of speed and accuracy of networks with different number of hidden layers and activation functions\n",
    "\n",
    "Done in a separate notebook with results presented below.\n",
    "\n",
    "All 12 networks were trained on multimodal-large dataset with a total of 120 hidden neurons, batch size of 1, and 200 epochs. Learning rate was selected manually for best results. Total execution time: 42min on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>hidden</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1602.898149</td>\n",
       "      <td>1566.403175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>144.593680</td>\n",
       "      <td>143.693841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>7.606620</td>\n",
       "      <td>12.495676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4459.693009</td>\n",
       "      <td>4424.354227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4583.188541</td>\n",
       "      <td>4548.167808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>linear</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4649.009926</td>\n",
       "      <td>4613.984240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>4.527554</td>\n",
       "      <td>9.502291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1583.002296</td>\n",
       "      <td>1445.241671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>416.797194</td>\n",
       "      <td>374.565892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tanh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3.368971</td>\n",
       "      <td>8.402341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>737.841324</td>\n",
       "      <td>727.637919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1152.339918</td>\n",
       "      <td>1042.079657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation  hidden  learning rate   loss_train    loss_test\n",
       "0        relu       1         0.0003  1602.898149  1566.403175\n",
       "1        relu       2         0.0003   144.593680   143.693841\n",
       "2        relu       3         0.0001     7.606620    12.495676\n",
       "3      linear       1         0.0001  4459.693009  4424.354227\n",
       "4      linear       2         0.0001  4583.188541  4548.167808\n",
       "5      linear       3         0.0001  4649.009926  4613.984240\n",
       "6     sigmoid       1         0.0030     4.527554     9.502291\n",
       "7     sigmoid       2         0.0003  1583.002296  1445.241671\n",
       "8     sigmoid       3         0.0003   416.797194   374.565892\n",
       "9        tanh       1         0.0010     3.368971     8.402341\n",
       "10       tanh       2         0.0003   737.841324   727.637919\n",
       "11       tanh       3         0.0001  1152.339918  1042.079657"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('lab6_model_selection_results.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - steps-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x    y\n",
      "1  1.706990  160\n",
      "2 -0.604580  -80\n",
      "3 -0.674405  -80\n",
      "4  1.341562   80\n",
      "5 -1.427434  -80\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/regression/steps-large-training.csv',index_col=0)\n",
    "df_test = pd.read_csv('data/regression/steps-large-test.csv',index_col=0)\n",
    "print(df_test.head())\n",
    "\n",
    "x_train = [[x] for x in df_train.loc[:,\"x\"]]\n",
    "y_train = [[y] for y in df_train.loc[:,\"y\"]]\n",
    "x_test = [[x] for x in df_test.loc[:,\"x\"]]\n",
    "y_test = [[y] for y in df_test.loc[:,\"y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/100,  MSE loss train:   148.31,  test:  153.276\n",
      "Epoch:   11/100,  MSE loss train:   56.259,  test:    50.93\n",
      "Epoch:   21/100,  MSE loss train:   44.133,  test:   38.191\n",
      "Epoch:   31/100,  MSE loss train:    38.07,  test:   31.991\n",
      "Epoch:   41/100,  MSE loss train:   34.409,  test:   27.908\n",
      "Epoch:   51/100,  MSE loss train:    32.06,  test:   25.053\n",
      "Epoch:   61/100,  MSE loss train:    30.26,  test:   22.768\n",
      "Epoch:   71/100,  MSE loss train:   29.014,  test:   21.086\n",
      "Epoch:   81/100,  MSE loss train:   28.101,  test:   19.797\n",
      "Epoch:   91/100,  MSE loss train:   27.356,  test:   18.744\n",
      "Epoch:  100/100,  MSE loss train:   26.665,  test:   17.719\n"
     ]
    }
   ],
   "source": [
    "net = mlp.NeuralNetwork()\n",
    "net.add(mlp.Layer(neurons_count=1, add_bias=True))\n",
    "net.add(mlp.Layer(neurons_count=30, activation_fun=mlp.ActivationTanh(), add_bias=True))\n",
    "net.add(mlp.Layer(neurons_count=1, add_bias=False))\n",
    "net.train(x_train, y_train,x_test,y_test, epochs=100, learning_rate=0.001, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/100,  MSE loss train: 3300.485,  test: 3243.998\n",
      "Epoch:   11/100,  MSE loss train:   412.03,  test:  428.202\n",
      "Epoch:   21/100,  MSE loss train:  265.872,  test:  283.168\n",
      "Epoch:   31/100,  MSE loss train:  226.766,  test:  243.377\n",
      "Epoch:   41/100,  MSE loss train:   85.135,  test:   94.242\n",
      "Epoch:   51/100,  MSE loss train:   64.996,  test:   65.314\n",
      "Epoch:   61/100,  MSE loss train:   25.996,  test:    33.02\n",
      "Epoch:   71/100,  MSE loss train:   76.055,  test:   74.339\n",
      "Epoch:   81/100,  MSE loss train:  277.262,  test:  281.756\n",
      "Epoch:   91/100,  MSE loss train:   10.903,  test:    17.01\n",
      "Epoch:  100/100,  MSE loss train:    7.981,  test:   13.861\n"
     ]
    }
   ],
   "source": [
    "net = mlp.NeuralNetwork()\n",
    "net.add(mlp.Layer(neurons_count=1, add_bias=True))\n",
    "net.add(mlp.Layer(neurons_count=30, activation_fun=mlp.ActivationReLU(), add_bias=True))\n",
    "net.add(mlp.Layer(neurons_count=30, activation_fun=mlp.ActivationReLU(), add_bias=True))\n",
    "net.add(mlp.Layer(neurons_count=30, activation_fun=mlp.ActivationReLU(), add_bias=True))\n",
    "net.add(mlp.Layer(neurons_count=1, add_bias=False))\n",
    "net.train(x_train, y_train,x_test,y_test, epochs=100, learning_rate=0.0001, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - rings5-regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              x          y  c\n",
      "706  -97.899164  17.106432  4\n",
      "460  -98.035256 -28.845421  3\n",
      "1234  98.816094 -93.602525  3\n",
      "833   46.096214 -19.163398  1\n",
      "1062  16.987171 -93.895154  2\n",
      "\n",
      "Unique classes: 5\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/classification/rings5-regular-training.csv').sample(frac=1)\n",
    "df_test = pd.read_csv('data/classification/rings5-regular-test.csv').sample(frac=1)\n",
    "print(df_test.head())\n",
    "\n",
    "# onehot encoding\n",
    "x_train = df_train.loc[:,df_train.columns!='c'].to_numpy().tolist()\n",
    "y_train = pd.get_dummies(df_train.loc[:,df_train.columns=='c'].squeeze(axis=1), prefix='class').to_numpy().tolist()\n",
    "x_test = df_test.loc[:,df_test.columns!='c'].to_numpy().tolist()\n",
    "y_test = pd.get_dummies(df_test.loc[:,df_test.columns=='c'].squeeze(axis=1), prefix='class').to_numpy().tolist()\n",
    "\n",
    "print(f\"\\nUnique classes: {np.array(y_train).shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matem\\PycharmProjects\\miowad\\MultiLayerPerceptron.py:279: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = TP/(TP+FP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/100,  MSE loss train:    0.509,  test:    0.488   |   F1 macro train:    0.318,  test:    0.298\n",
      "Epoch:   11/100,  MSE loss train:    0.106,  test:     0.12   |   F1 macro train:    0.641,  test:    0.571\n",
      "Epoch:   21/100,  MSE loss train:    0.099,  test:    0.111   |   F1 macro train:    0.686,  test:    0.623\n",
      "Epoch:   31/100,  MSE loss train:    0.093,  test:    0.107   |   F1 macro train:    0.695,  test:    0.617\n",
      "Epoch:   41/100,  MSE loss train:    0.089,  test:    0.103   |   F1 macro train:    0.709,  test:    0.623\n",
      "Epoch:   51/100,  MSE loss train:    0.087,  test:      0.1   |   F1 macro train:     0.73,  test:    0.646\n",
      "Epoch:   61/100,  MSE loss train:    0.087,  test:    0.098   |   F1 macro train:    0.718,  test:    0.656\n",
      "Epoch:   71/100,  MSE loss train:    0.083,  test:    0.096   |   F1 macro train:     0.75,  test:    0.672\n",
      "Epoch:   81/100,  MSE loss train:    0.081,  test:    0.095   |   F1 macro train:    0.748,  test:    0.676\n",
      "Epoch:   91/100,  MSE loss train:     0.08,  test:    0.094   |   F1 macro train:    0.752,  test:    0.686\n",
      "Epoch:  100/100,  MSE loss train:    0.079,  test:    0.092   |   F1 macro train:     0.76,  test:      0.7\n"
     ]
    }
   ],
   "source": [
    "net = mlp.NeuralNetwork()\n",
    "net.add(mlp.Layer(2))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationReLU()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationReLU()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationReLU()))\n",
    "net.add(mlp.Layer(5, activation_fun=mlp.ActivationTanh(), add_bias=False))\n",
    "net.train(x_train, y_train, x_test, y_test, epochs=100, learning_rate=0.01, batch_size=10, loss_function=mlp.LossMSE(f1_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/100,  MSE loss train:    0.148,  test:    0.157   |   F1 macro train:    0.328,  test:    0.246\n",
      "Epoch:   11/100,  MSE loss train:    0.131,  test:    0.145   |   F1 macro train:    0.404,  test:    0.333\n",
      "Epoch:   21/100,  MSE loss train:    0.128,  test:    0.142   |   F1 macro train:    0.458,  test:    0.381\n",
      "Epoch:   31/100,  MSE loss train:    0.125,  test:    0.141   |   F1 macro train:     0.47,  test:    0.389\n",
      "Epoch:   41/100,  MSE loss train:    0.124,  test:     0.14   |   F1 macro train:    0.478,  test:    0.389\n",
      "Epoch:   51/100,  MSE loss train:    0.121,  test:    0.138   |   F1 macro train:    0.498,  test:    0.372\n",
      "Epoch:   61/100,  MSE loss train:     0.12,  test:    0.137   |   F1 macro train:    0.505,  test:    0.367\n",
      "Epoch:   71/100,  MSE loss train:    0.119,  test:    0.136   |   F1 macro train:     0.51,  test:     0.37\n",
      "Epoch:   81/100,  MSE loss train:    0.118,  test:    0.136   |   F1 macro train:    0.522,  test:    0.396\n",
      "Epoch:   91/100,  MSE loss train:    0.117,  test:    0.136   |   F1 macro train:    0.532,  test:    0.406\n",
      "Epoch:  100/100,  MSE loss train:    0.116,  test:    0.135   |   F1 macro train:    0.529,  test:    0.407\n"
     ]
    }
   ],
   "source": [
    "net = mlp.NeuralNetwork()\n",
    "net.add(mlp.Layer(2))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationTanh()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationTanh()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationTanh()))\n",
    "net.add(mlp.Layer(5, activation_fun=mlp.ActivationTanh(), add_bias=False))\n",
    "net.train(x_train, y_train, x_test, y_test, epochs=100, learning_rate=0.01, batch_size=10, loss_function=mlp.LossMSE(f1_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - rings3-regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              x          y  c\n",
      "1589  93.057692 -51.823544  2\n",
      "100   48.683090  83.667869  1\n",
      "1184  59.250966  13.685368  1\n",
      "609    1.597480 -47.192650  0\n",
      "867   97.458982  61.258681  2\n",
      "\n",
      "Unique classes: 3\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/classification/rings3-regular-training.csv').sample(frac=1)\n",
    "df_test = pd.read_csv('data/classification/rings3-regular-test.csv').sample(frac=1)\n",
    "print(df_test.head())\n",
    "\n",
    "# onehot encoding\n",
    "x_train = df_train.loc[:,df_train.columns!='c'].to_numpy().tolist()\n",
    "y_train = pd.get_dummies(df_train.loc[:,df_train.columns=='c'].squeeze(axis=1), prefix='class').to_numpy().tolist()\n",
    "x_test = df_test.loc[:,df_test.columns!='c'].to_numpy().tolist()\n",
    "y_test = pd.get_dummies(df_test.loc[:,df_test.columns=='c'].squeeze(axis=1), prefix='class').to_numpy().tolist()\n",
    "\n",
    "print(f\"\\nUnique classes: {np.array(y_train).shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/100,  MSE loss train:     0.66,  test:    0.663   |   F1 macro train:    0.305,  test:    0.303\n",
      "Epoch:   11/100,  MSE loss train:    0.171,  test:    0.175   |   F1 macro train:    0.591,  test:    0.579\n",
      "Epoch:   21/100,  MSE loss train:    0.157,  test:    0.162   |   F1 macro train:    0.669,  test:    0.647\n",
      "Epoch:   31/100,  MSE loss train:    0.147,  test:    0.155   |   F1 macro train:    0.719,  test:     0.69\n",
      "Epoch:   41/100,  MSE loss train:    0.133,  test:    0.142   |   F1 macro train:    0.738,  test:    0.719\n",
      "Epoch:   51/100,  MSE loss train:     0.13,  test:    0.141   |   F1 macro train:    0.742,  test:    0.712\n",
      "Epoch:   61/100,  MSE loss train:     0.13,  test:    0.144   |   F1 macro train:    0.737,  test:    0.704\n",
      "Epoch:   71/100,  MSE loss train:    0.127,  test:    0.142   |   F1 macro train:    0.738,  test:    0.712\n",
      "Epoch:   81/100,  MSE loss train:    0.114,  test:    0.126   |   F1 macro train:    0.784,  test:    0.759\n",
      "Epoch:   91/100,  MSE loss train:    0.106,  test:    0.117   |   F1 macro train:    0.795,  test:    0.775\n",
      "Epoch:  100/100,  MSE loss train:    0.106,  test:    0.119   |   F1 macro train:     0.79,  test:    0.774\n"
     ]
    }
   ],
   "source": [
    "net = mlp.NeuralNetwork()\n",
    "net.add(mlp.Layer(2))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationReLU()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationReLU()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationReLU()))\n",
    "net.add(mlp.Layer(3, activation_fun=mlp.ActivationTanh(), add_bias=False))\n",
    "net.train(x_train, y_train, x_test, y_test, epochs=100, learning_rate=0.01, batch_size=10, loss_function=mlp.LossMSE(f1_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/100,  MSE loss train:    0.202,  test:    0.198   |   F1 macro train:     0.49,  test:    0.509\n",
      "Epoch:   11/100,  MSE loss train:     0.19,  test:    0.188   |   F1 macro train:    0.515,  test:    0.548\n",
      "Epoch:   21/100,  MSE loss train:    0.184,  test:    0.184   |   F1 macro train:    0.546,  test:    0.543\n",
      "Epoch:   31/100,  MSE loss train:    0.178,  test:    0.177   |   F1 macro train:    0.576,  test:    0.565\n",
      "Epoch:   41/100,  MSE loss train:    0.177,  test:    0.177   |   F1 macro train:    0.563,  test:    0.558\n",
      "Epoch:   51/100,  MSE loss train:    0.169,  test:    0.168   |   F1 macro train:     0.58,  test:     0.57\n",
      "Epoch:   61/100,  MSE loss train:    0.175,  test:    0.174   |   F1 macro train:    0.576,  test:    0.554\n",
      "Epoch:   71/100,  MSE loss train:    0.158,  test:    0.164   |   F1 macro train:    0.609,  test:    0.598\n",
      "Epoch:   81/100,  MSE loss train:    0.154,  test:     0.16   |   F1 macro train:    0.622,  test:    0.604\n",
      "Epoch:   91/100,  MSE loss train:    0.144,  test:    0.153   |   F1 macro train:    0.668,  test:    0.642\n",
      "Epoch:  100/100,  MSE loss train:    0.144,  test:    0.153   |   F1 macro train:    0.665,  test:    0.639\n"
     ]
    }
   ],
   "source": [
    "net = mlp.NeuralNetwork()\n",
    "net.add(mlp.Layer(2))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationTanh()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationTanh()))\n",
    "net.add(mlp.Layer(20, activation_fun=mlp.ActivationTanh()))\n",
    "net.add(mlp.Layer(3, activation_fun=mlp.ActivationTanh(), add_bias=False))\n",
    "net.train(x_train, y_train, x_test, y_test, epochs=100, learning_rate=0.01, batch_size=10, loss_function=mlp.LossMSE(f1_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Additional question regarding ReLU gradient implementation\n",
    "\n",
    "ReLU function may have differences in gradient implementation - some may set gradient to 0 for negative values, some may set it as 0.1 for example (called Leaky ReLu).\n",
    "\n",
    "### Comparison of speed and accuracy of networks with different number of hidden layers and activation functions\n",
    "\n",
    "Done in a separate notebook with results presented above.\n",
    "\n",
    "In general, the best results were obtained by networks with 1 hidden layer with 120 neurons and nonlinear activation functions, but also with 3 hidden layers and ReLU activation function.\n",
    "\n",
    "Linear activation function is useless as no matter how many layers and neurons, the output will always be a linear function.\n",
    "\n",
    "It's also worth noticing that ReLU and tanh activation functions are faster than the sigmoid activation function. The cause of this is that ReLU and tanh activation functions are implemented in numpy, which turns out to be much faster than a vectorized python function implemented for sigmoid.\n",
    "\n",
    "### Performance on other datasets\n",
    "\n",
    "#### Regression - steps-large\n",
    "\n",
    "Model 1 - 1x30 neurons, tahn activation function (on output layer linear activation)\n",
    "- mse train:    28.101\n",
    "- mse test:     19.797\n",
    "\n",
    "Model 2 - 3x30 neurons, ReLU activation function (on output layer linear activation)\n",
    "- mse train:    7.981\n",
    "- mse test:     13.861\n",
    "\n",
    "#### Classification - rings5-regular\n",
    "\n",
    "Model 1 - 3x20 neurons, relu activation function (on output layer tahn activation)\n",
    "- f1 train: 0.76     \n",
    "- f1 test:  0.7\n",
    "\n",
    "Model 2 - 3x20 neurons, tanh activation function (on output layer too)\n",
    "- f1 train: 0.529\n",
    "- f1 test:  0.407\n",
    "\n",
    "#### Classification - rings3-regular\n",
    "\n",
    "Model 1 - 3x20 neurons, relu activation function (on output layer tahn activation)\n",
    "- f1 train: 0.79\n",
    "- f1 test:  0.774\n",
    "\n",
    "Model 2 - 3x20 neurons, tanh activation function (on output layer too)\n",
    "- f1 train: 0.665\n",
    "- f1 test:  0.639\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6553f9dcd9d8af4a6d8fef01a514c9a715cb88eedc2258fed476c62d27d9dcbf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
